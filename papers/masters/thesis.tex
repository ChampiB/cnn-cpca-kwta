\documentclass[11pt]{report}
\usepackage[backend=bibtex,style=numeric]{biblatex}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[margin=1in]{geometry}
\usepackage[section]{placeins}
\usepackage{csquotes}
\usepackage{titlesec}
\usepackage[acronym]{glossaries}
\usepackage{mathtools}
\usepackage{minted}
\usepackage{titlepic}
\usepackage{graphicx}

\newcommand{\dd}[1]{\mathrm{d}#1}

\bibliography{references}

\title{\textbf{Calculus on Computational Graphs and Image Analysis}}
\author{
\begin{tabular}[t]{cc}
Champion Th√©ophile \\
tmac2@kent.ac.uk \\
10559 words\\
\end{tabular}
}
\date{\today}
\titlepic{\includegraphics[width=7cm]{uni_kent}}

\makeglossaries

\newglossaryentry{task_learning}
{
    name=Task learning,
    description={Task or supervised learning refer to the algorithms that aims to solve a task. In task learning, the expected output is known and part of the expected mapping is given to the models during the learning}
}
\newglossaryentry{model_learning}
{
    name=Model learning,
    description={Model or unsupersised learning refer to the algorithms that aims to create a model of the data. In model learning, the expected output is unknown and the models search for correlations, clusters or structures in the data}
}
\newglossaryentry{sparsity}
{
    name=Sparsity,
    description={Sparsity refer to something thinly scattered or distributed. In the context of neural networks, the term sparse respresentation is used to describe a neural network when most of its weights are zero}
}
\newglossaryentry{feature_map}
{
    name=Feature Map,
    description={A feature map is a 2D array, where each element is generated by computing the kernel's activation at one position in the previous layer}
}
\newglossaryentry{overlapping_kernel}
{
    name=Overlapping pooling,
    description={The mean and max pooling layers are overlapping if the strides and the pooling size are different}
}
\newglossaryentry{tflo_per_sec}
{
    name=TFLOPS,
    description={The teraflop is used to measure computational power. One teraflop is the ability of a computer to handle one trillion of floating-point calculations every second}
}
\newglossaryentry{ReLu}
{
    name=ReLu,
    description={The ReLu activation function have been introduce to solve the vanishing gradient problem in neural network. It is defined by: $ReLu(x) = max(0, x)$}
}
\newglossaryentry{sigmoid}
{
    name=Sigmoid,
    description={The sigmoid function was one of the first activation to be used in neural network. It is defined by: $sigmoid(x) = \frac{1}{1 + e^{-x}}$}
}
\newglossaryentry{softmax}
{
    name=Softmax,
    description={The softmax function is commonly used as an activation function for the output layer of a network. It is defined by: $softmax(a_i) = \frac{e^{a_i}}{\sum_{k}{e^{a_k}}}$}
}
\newglossaryentry{vanishing_gradient_problem}
{
    name=Vanishing gradient problem,
    description={In the early days of deep leatning the sigmoid activation function was frequently used. This function lead the gradients to get smaller and smaller as they are backpropagate through the layers; leaving the networks unable to learn, i.e. vanishing gradient problem}
}
\newglossaryentry{batch}
{
    name=Batch training,
    description={The computational time required to compute the error of a network on the entire dataset grows with the number of examples. To avoid this expensive computation, the batch gradient descent process the dataset by batch of $k$ exemples}
}
\newglossaryentry{MNIST}
{
    name=MNIST,
    description={The MNIST dataset is a database of (28 x 28) pixel image of handwritten digits. There are 60000 train images and 10000 test images. There are 10 classes, i.e. one per digit}
}

\newacronym{cnn}{CNN}{Convolutional Neural Network}
\newacronym{conv_net}{ConvNet}{Convolutional Neural Network}
\newacronym{cpca}{CPCA}{Conditional Principal Component Analysis}
\newacronym{kwta}{kWTA}{k-Winners-Take-All}
\newacronym{hcnn}{HCNN}{Hebbian Convolutional Neural Network}
\newacronym{cpu}{CPU}{Central Processing Unit}
\newacronym{gpu}{GPU}{Graphics Processing Unit}
\newacronym{std}{STD}{Standard Deviation}
\newacronym{api}{API}{Application Programming Interface}
\newacronym{tflops}{TFLOPS}{Tera floating-point operations per second}


\begin{document}

\maketitle

\tableofcontents

\glsaddall
\printglossary[type=\acronymtype, nonumberlist]
\addcontentsline{toc}{chapter}{Acronyms}
\printglossary[type=main, nonumberlist]
\addcontentsline{toc}{chapter}{Glossary}

\listoffigures
\addcontentsline{toc}{chapter}{List of Figures}
\listoftables
\addcontentsline{toc}{chapter}{List of Tables}

\newpage

\chapter{Introduction}

Image analysis is the field that aims to extract meaning from images. This is an important area of research in machine learning, which has found application from the reading of bar coded tags to the detection of cancerous tumors. It is needless to say that any improvement of the state-of-the-art will have an impact on a lot of domains. This thesis proprose new ideas to improve the convolutional neural network, which is the most popular model in the field.

\section{Objectives}

The first objective of this paper is to propose a variation of the well known convolutional layer, which uses a mixture of task and model learning. This variation will be compared with the original convolutional layer in terms of sparsity of the weights learned, computational time and accuracy.
\newline
\newline
\noindent The second aims of this paper is to implement it from scratch. This means recoding the graph logic of TensorFlow and being able to backpropagate the gradients through any non cyclic graph of computation. The flexibility provided by such logic will enable the implementation of state-of-the-art models used in computer vision.
\newline
\newline
\noindent The third goal of this project is to show how general purpose GPU programming can speed up neural networks. This will be done by comparing the CPU and the GPU implementation of various layers, such as max pooling and convolutional layers.

\section{Overview of Contents}

\paragraph{Chapter \ref{related_work}:} Present image analysis techiques as well as notions relevant to the chapter \ref{model_and_task_learning}.
\paragraph{Chapter \ref{model_and_task_learning}:} Propose a new model that learn from the structure of the data.
\paragraph{Chapter \ref{model_implementation}:} Dive into the implementation details of the model.
\paragraph{Chapter \ref{result_and_analysis}:} Analyse the model in terms of accuracy and learning speed.
\paragraph{Chapter \ref{framework_limitation}:} Explain the current limitation of the framework.
\paragraph{Chapter \ref{conclusion}:} Conclude the paper and propose future research directions.

\chapter{Realted Work} \label{related_work}


\section{Deep \acrshort{conv_net}}

This section aims to motivate the improvement of the \acrshort{conv_net} by giving an overview of the variety of model based on it and the multitude of application domains.

\paragraph{Object detection:} Object detection is the field dedictated to the recognition of objects inside images. Over the recent years many models have been proposed to pull the error on the ImageNet dataset \cite{imagenet_cvpr09} down. It started with the VGGNet \cite{DBLP:journals/corr/RussakovskyDSKSMHKKBBF14} and the AlexNet \cite{DBLP:journals/cacm/KrizhevskySH17}, which are simply deeper \acrshort{cnn}s. Then the GoogleNet \cite{DBLP:conf/cvpr/SzegedyLJSRAEVR15} introduced the Inception Module, which is able to extract multi-level features from the same input. Indeed, the Inception Module is using kernels of various size, i.e. $1\times1$, $3\times3$ and $5\times5$, to extract features from its inputs. Finally, the ResNet architecture \cite{DBLP:conf/cvpr/HeZRS16} introduced the idea of skip connections where some connections are directly linked to other layers much higer in the hierachy. The lastest achieved a top-5 error rate of 3.57\% on the ImageNet dataset, which is composed of 1000 classes of objects.\newline

\noindent All those models shared the same building blocks (i.e. convolution and pooling layers) and have been successfully applied to the extraction of text from images, face recognition and scence understanding for autonomous vehicles.

\paragraph{Image segmentation:} Image segmentation is another important task in computer vision, which constists of a pixelwise binary prediction. The model is trained to predict one for all pixels that belongs to a class and zero otherwise. The state-of-the-art in image segmentation are architecture like UNet \cite{DBLP:conf/miccai/RonnebergerFB15} and LinkNet \cite{DBLP:conf/vcip/ChaurasiaC17}. Those architectures are composed of two parts, i.e. an encoder and a decoder. The encoder tranform the input image into a simpler and more compact representation that can then be unfold by the decoder to produce the final output.\newline

\noindent Needless to say that image segmentation models can be used to precisely determined the boundary of an object. Another advantage of this technique is that it can be used in collaboration with classical object detection to improve the accuracy of both the segmentation and the classification \cite{DBLP:conf/cvpr/KendallGC18}. The idea is that the encoder remains the same but several tasks are learned from it. The image segmentation model used the encoder as before while the object detection model used the features that it produce to performs object detection.

\section{Convolutional Neural Network}

The \acrshort{cnn} or \acrshort{conv_net} architecture \cite{NIPS1989_293} is a deep neural network learning by backpropagation. It is composed of two kind of layers: convolution and pooling. The convolution layer contains kernels that will be slide through the image with step size defined by the strides. Afterwards, non-linear activation is added after the weighted sum produce by the kernels \cite{5537907} such as:
\begin{equation}
y_{convolution} = \sigma(z)\ \ \ with\ \ \ z = \sum_{i} x_i \times w_i
\end{equation}
where $y$ is the kernel's output for a specific position in the image, $i$ iterate through all the input as show by the indexes in figure \ref{fig:conv_net}, $x$ are the inputs, $w$ are the kernel's weights and $\sigma$ is the non-linear activation function. Also, the use of local normalisation allows local competition between the kernels. The pooling layer allow to reduce the dimensionality and making the model invariant to small local variation of its inputs. In the original version \textcite{NIPS1989_293} the pooling applied local average, i.e. mean pooling. More recently, the max pooling have shown better results \cite{10.1007/978-3-642-15825-4_10} by taking the highest value from the pooling kernel such as:
\begin{equation}
y_{mean}\ =\ \frac{1}{|x|}\ \times\ \sum_{i} x_i\ \ \ and\ \ \ y_{max}\ =\ \max_{i}\ x_i
\end{equation}
where $y_{mean}$ and $y_{max}$ are the output of the mean and max pooling for a specific position in the image, $i$ iterate through all the inputs of the pooling kernel, $x$ are the inputs of the pooling kernel and $|x|$ is the number of inputs in the pooling kernel. Finally, \textcite{DBLP:journals/corr/abs-1811-12231} recently show that CNNs are bias towards texture and describe a method based on image style transfer \cite{Gatys_2016_CVPR}, which increase the shape bias as well as the accuracy.

\begin{figure}[h]
\centering
\includegraphics[width=11cm, height=7cm]{conv_net}
\caption[Illustration of convolution and max pooling layers]{This figure illustrates the convolution and pooling nodes of the \acrshort{conv_net}. In this example there is two kernels of size $2\times2$ with stride $1\times1$, which produce two feature maps. Each element in the feature maps are produced by taking the weighted sum of the kernel's input and passing the result through a non-linear activation function. Then the $2\times2$ max pooling is applied to each feature map to produce the final output.}
\label{fig:conv_net}
\end{figure}

\section{Conditional Principal Component Analysis}

The following explanation are based on the chapter 4 of \textcite{OReilly:2000:CEC:557205:chap4}. Hebbian learning is a biologically plausible and unsupervised learning thechnique based on the idea that neurons that trigger together must strengthen their connection. The simplest learning rule in Hebbian learning is as follow:

\begin{equation}
\Delta w_{ij} = \epsilon y_jx_i\ \ \ with\ \ \ y_j = \sum_{i}{x_i \times w_{ij}}
\end{equation}

\noindent where $\epsilon$ is the learning rate, $x_i$ is the i-th input, $y_j$ is the output of the j-th neuron, $w_{ij}$ is the i-th weight of the j-th neuron and $\Delta w_{ij}$ is the update for the weight $w_{ij}$. It can be show that this learning rule learned the first principal component of the data, but this technique has many limitations. First, the weights will become infinitly large as the neurons learn. This can be fix by using the Oja's rule, which normalise the weights update:

\begin{equation}
\Delta w_{ij} = \epsilon (x_iy_j - y_j^2w_{ij})
\end{equation}

\noindent Second, if many neurons are trained using the Oja's rule they will all learn the same component. This can be fix by using the Sanger's rule, which substract the information already explained by the first components:

\begin{equation}
\Delta w_{ij} = \epsilon (x_{Stranger}y_j - y_j^2w_{ij})\ \ \ with\ \ \ x_{Stranger} = x_i - \sum_{k = 1}^{j - 1}{y_kw_{ik}}
\end{equation}

\noindent Finally, all of the previous learning rules compute the principal component over all the data points. This become a problem when the data is very heterogeneous. Indeed, in the case of the figure \ref{fig:heterogeneous_data} (a) the component learned by such methods will correspond to the blue line, which does not represent well the shape of the data. In this case, it is preferable to learn from only a subset of the examples, i.e. only one of the two clusters, as shows in figure \ref{fig:heterogeneous_data} (b) where the bottom-most cluster is ignored. The \acrshort{cpca} is based on this idea and the learning relies on a conditioning function that enable each neuron to learn only for a subset of the data. The conditioning function used in this paper will be explained in the next section. The learning rule in \acrshort{cpca} is defined as follow:

\begin{equation}
\Delta w_{ij} = \epsilon y_j (x_i - w_{ij})
\end{equation}

\noindent This learning rule force each weight $w_{ij}$ to tends to represent the probability of $x_i$ given that the neuron active, i.e. $w_{ij} = P(x_i = 1 | y_j = 1)$. Note that the activation of the ReLU function can potentially be infinite. For this reason, the activation of each neuron is truncated before the computation of the Hebbian learning rule, i.e. the activation used to compute the Hebbian gradients is never bigger than one.

\begin{figure}[h]
\centering
\includegraphics[width=15cm, height=5cm]{heterogeneous_data}
\caption[Illustration of data points belonging to two clusters]{This figure shows data points belonging to two clusters. On the right the first principal component is computed over all the data points. On the left the bottom-most cluster is ignored and the first principal component is computed only over the data points of the top-most cluster.}
\label{fig:heterogeneous_data}
\end{figure}

\section{K-Winner-Take-All} \label{sec:kwta}

The need for a conditioning function in \acrshort{cpca} have been highlighted in the previous section. This section presents the k-Winner-Take-All method that enable such conditioning. The k-Winner-Take-All aims to make only a subset of $k$ neurons to be active for each input. This can be achieved by either inhibiting all the neurons until only $k$ of them remains active or by forcing the activation of $n - k$ neurons to zero, where $n$ is the total number of neuron in the node.\newline

\noindent Independently of the method chosen only the $k$ neurons which have the highest activation will remain active. Those neurons are the ones whose weights fitted the best the input pattern. Since, the learning rule in \acrshort{cpca} is making the weights tends to $P(x_i = 1 | y_j = 1)$, then those neurons will fit the input even better. Finally, the last point of this section will be to explain how \acrshort{cpca} and \acrshort{kwta} are related to the figure \ref{fig:heterogeneous_data} (b). By now, it should be clear that \acrshort{kwta} will enable a neuron to ignore the bottom-most cluster if its weights does not fit it well. Additionally, the \acrshort{cpca} learning rule will allows the neuron to both fit the top-most cluster even better and learn a useful representation of the input for which it is reponsible.

\chapter{Mixing Model and Task Learning} \label{model_and_task_learning}

\section{Motivations}

The motivation behind a mixture of task and model learning, directly came from the advantages and disadvantages of each methods as explained in chapter 6 of \textcite{OReilly:2000:CEC:557205}. As an example, let us consider the Hebbian learning model, which belong to the model learning algorithms. In Hebbian learning, the units are learning local correlational structure of their inputs. This has the advantages of being reliable and autonomous. Indeed, the units are learning from the data (i.e. reliable) and does not depend on the activation of the neurons upper in the hiearachy (i.e. autonomous). However, this model suffer from its lack of vision and is unable to update its weights for serving a greater good such as solving a task, which is where the task learning algorithms come into play. The most popular task learning algorithm is the backpropagation algorithm. It has the advantages of being task-driven and cooperative. Indeed, the units are working together (i.e. cooperative) in order to solve a task (i.e. task-driven). On the other hand, the backpropagation is lazy as it stop learning when the task have been solved and the interdependency can slow down the learning as the gradients coming from different output units can balance each other out. To sum up, the model-based algorithm might enable the learning of reliable features and speed up the task learning by framing the model based on the incoming data.

\section{\acrlong{hcnn}}

\subsection{Mixing Hebbian learning and Backpropagation} \label{sec:mix_hebb_bp}

The idea of mixing Hebbian learning and backpropagation have already been proposed by \textcite{OReilly:2000:CEC:557205}, where the authors shown how to combine backpropagation and Hebbian learning such as:
\begin{equation}
\Delta_{weights} = \epsilon (ratio \times \Delta_{Hebbian} + (1 - ratio) \times \Delta_{backpropagation})
\end{equation}
where $\Delta_{weights}$ is the final update of the weights, $\epsilon$ is the learning rate, $ratio$ is the amount of Hebbian learning between zero and one, $\Delta_{Hebbian}$ is Hebbian update of the weights and $\Delta_{backpropagation}$ is backpropagation update of the weights. Finally, \textcite{OReilly:2000:CEC:557205} explained that a reasonable value for the $ratio$ of Hebbian is between $0.0005$ and $0.01$.

\subsection{\acrshort{cnn} with \acrshort{cpca} and \acrshort{kwta}}
The previous subsection explained how to mix Hebbian learning and backpropagation. Also, this paragraph describe a new model, i.e. \acrshort{hcnn}, obtained by mixing a \acrshort{cnn} with \acrshort{cpca} and \acrshort{kwta}. The only layer modified in this model is the convolutional layer. As explained in section \ref{sec:mix_hebb_bp} the final weights update will be a mix between the \acrshort{cpca} and the backpropagation gradients. Additionaly, as explained in section \ref{sec:kwta} \acrshort{cpca} required a conditioning function to learn efficiently. This will be achieve using a \acrshort{kwta} competition between the neurons of the convolutional node. Figure \ref{fig:hcnn} shows the convolutional layer with \acrshort{cpca} and \acrshort{kwta}.

\begin{figure}[h]
\centering
\includegraphics[width=15cm, height=10cm]{hcnn}
\caption[Illustration of the \acrlong{hcnn}]{This figure illustrate the convolutional layer of the \acrlong{hcnn}. The inputs are the same for all the kernel. The weights are used to computed the weighted sum of the inputs and the result is passed through a non-linear activation function. Then, the \acrshort{kwta} ensure that only two neurons win the competition and set the activation of the orange kernel to zero. Finally, the gradients of the backpropagation and \acrshort{cpca} are computed and mixed using the equation presented in section \ref{sec:mix_hebb_bp}.}
\label{fig:hcnn}
\end{figure}

\noindent In this model the \acrshort{kwta} competition allows the \acrshort{cpca} to learn more efficiently, i.e. conditioning function. It also brings more stability in the learning as only a subset of the neuron learn, i.e. change what they represent and react to. The \acrshort{cpca} on the other side add data-driven constraint on the way the weights are learned. This aims to avoid unrealistic pattern to be learned and can be seen as a regularisation of the weights. Indeed, the equation of weights update, i.e. $\Delta w_{ij} = \epsilon y_j (x_i - w_{ij})$, shows that as the weights become large the difference between the data and weights, i.e. $x_i - w_{ij}$, become large and negative. This elegantly force the weights to remain small.

\chapter{Model implementation} \label{model_implementation}

This chapter discuss the implementation of the \acrshort{hcnn} model. The implementation have been written from scratch and aims to provide an easily extendable framework. The code have been tested properly using unit tests and code running on the GPU have been written in order to achieve hardware acceleration. The code running on the \acrshort{cpu} have been written in Java and the code executed by the \acrshort{gpu} have been written using CUDA C.

\section{Datasets}

In order to make the framework easily extendable, the code provide a dataset interface, which allows to load the data by batch. Support for new datasets can easily be added by creating new classes that implement this interface. As soon as a new class have been implemented, the new dataset become available using the dataset factory. The following line of code create an instance of the MNIST dataset loading the example by batch of size twenty.

\begin{minted}[mathescape, linenos, numbersep=5pt,  frame=lines, framesep=2mm]{java}
DataSet dataSet = DataSetFactory.create("Mnist", 20);
\end{minted}

\noindent The dataset interface is composed of six functions:

\begin{minted}[mathescape, linenos, numbersep=5pt,  frame=lines, framesep=2mm]{java}
    /**
     * Reload the dataset.
     */
    public abstract void reload();

    /**
     * Reload the dataset.
     * @param training true if training set is required and false otherwise.
     */
    public abstract void reload(boolean training);

    /**
     * Check if there is a next batch.
     * @param training true if training set is required and false otherwise.
     * @return true if there is a next batch and false otherwise.
     */
    public abstract boolean hasNextBatch(boolean training);

    /**
     * Select the next batch in the training set.
     * @param training true if training set is required and false otherwise.
     */
    public abstract void nextBatch(boolean training);

    /**
     * Return the number of classes.
     * @return the number of classes.
     */
    public abstract int getNumberOfClasses();

    /**
     * Return the features corresponding to the current batch.
     * @param training true if training set is required and false otherwise.
     * @return the features.
     */
    public abstract ArrayPtr getFeatures(boolean training);

    /**
     * Return the labels corresponding to the current batch.
     * @param training true if training set is required and false otherwise.
     * @return the labels.
     */
    public abstract ArrayPtr getLabels(boolean training);
\end{minted}

\section{Nodes} \label{sec:nodes}

\subsection{Interface}

The code also provide a node interface, which can be used to add support for new nodes. As we will see in the next section, this generic interface allows to build complex graph of computation by linking the nodes with each other. The node interface is composed of six functions:

\begin{minted}[mathescape, linenos, numbersep=5pt, frame=lines, framesep=2mm]{java}
    /**
     * Compute the node activation.
     * @param training the mode (training vs testing).
     * @param x is the input.
     * @return the activation.
     */
    public abstract ArrayPtr activation(boolean training, ArrayPtr... x);

    /**
     * Update the weights.
     * @param lr the learning rate.
     * @param gradients the back propagation gradients from the upper nodes.
     * @return the gradients with respect to the node's inputs.
     */
    public abstract ArrayPtr[] update(double lr, ArrayPtr... gradients);

    /**
     * Save the node to the file.
     * @param kryo the kryo object.
     * @param output the kryo output.
     */
    public abstract void save(Kryo kryo, Output output);

    /**
     * Load weights from file.
     * @param kryo the kryo object.
     * @param input the kryo input.
     * @return this.
     */
    public abstract Node loadWeights(Kryo kryo, Input input);

    /**
     * Load node from file.
     * @param kryo the kryo object.
     * @param input the kryo input.
     * @return this.
     */
    public abstract Node load(Kryo kryo, Input input);

    /**
     * Display the node on the standard output.
     */
    public abstract void print();
\end{minted}

\noindent The framework currently support the following nodes:
\begin{itemize}
	\item \textbf{Dense} which is a fully connected node;
	\item \textbf{Conv2D} which is a 2D convolutional node supporting \acrshort{cpca} and  \acrshort{kwta};
	\item \textbf{MaxPooling2D} which is a 2D max pooling node;
	\item \textbf{Flatten} which transform a multi-dimensional data into a flat representation;
	\item \textbf{Merge2D} which merge many inputs into a single output;
	\item \textbf{Activation} which apply various activation functions such as sigmoid and softmax;
	\item \textbf{Add} which perform an element wise addition;
	\item \textbf{AvgPooling2D} which is a 2D mean pooling node;
	\item \textbf{Identity} which forward the input, i.e. identity node;
	\item \textbf{KWTA2D} which perform 2D K-Winners-Takes-All competition;
	\item \textbf{Pad2D} which apply padding on 2D data.
\end{itemize}

\noindent The following subsections focus on explaining the backpropagation of the most important nodes.

\subsection{Flatten}

The flatten node simply reshape the data for example if the input is a $3\times3$ matrix the node flatten it as a vector of size 9. During the backpropagation the node will receive the derivative of the error function with respect to the each output neuron, i.e. a vector of size 9. This node does not have any weights and will just return the derivative of the error function with respect to the input, i.e. the vector of size 9 is reshaped and a $3\times3$ matrix is returned.

\subsection{MaxPooling2D}

In the same way as the flatten layer, the max pooling layer does not have any weights to update. During the forward pass the node keep in memory the mask that will be used to backpropagate the gradients. This mask has the same shape as the input and contains a one if the cell was the highest value of the pooling kernel and zero otherwise. Assuming a $2\times2$ max pooling node and a $4\times4$ input matrix the node will output a $2\times2$ matrix and keep in memory a $4\times4$ mask matrix. During the backward pass the node received a $2\times2$ matrix containing the derivative of the error function with respect to the each output neuron. The gradient are backpropagate by using the mask, indeed, each pooling kernel of the mask is multiplied by the corresponding gradient as shown in figure \ref{fig:max_pooling_bp}.

\begin{figure}[h]
\centering
\includegraphics[width=4.5cm, height=5cm]{max_pooling_bp}
\caption[Backropagation in max pooling layer]{This figure illustrate the forward and backward pass of the max pooling node. The blue matrix is the input data, the green is the node output and the orange one is the mask corresponding to the input. Finally, the purple matrix is the matrix containing the derivative with respect to the output of the neurons and the red one contains the derivative with respect to the input.}
\label{fig:max_pooling_bp}
\end{figure}

\subsection{Dense}

The Dense node is more interresting, because of two reasons. First, the node has weights to update, which required to compute the derivative with respect to the weights in addition to the derivative with respect to the inputs. Second, the node has an activation function that makes the problem a bit more difficult. The first notion required to correctly explained the backpropagation in a Dense node is known as the chain rule, which is defined by:

\begin{equation}
\frac{\delta z}{\delta x} = \frac{\delta z}{\delta y} \times \frac{\delta y}{\delta x}
\end{equation}

\noindent where $z$ is a function of $y$ and $y$ a function of $x$. In a Dense node each neuron $j$ compute its net input $z_j$ as a weighted sum of its inputs such as:

\begin{equation}
z_j = \sum_{i}{x_i \times w_{ij}}
\end{equation}

\noindent where $z_j$ is the net input of the j-th neuron, $x_i$ is the i-th input and $w_{ij}$ is the i-th weight of the j-th neuron. The net input is then pass through an activation function $\sigma$ such as:

\begin{equation}
y_j = \sigma(z_j)
\end{equation}

\noindent where $y_j$ is the activation of the j-th neuron. During the backpropagation the node needs to determined the derivative of the error function with respect to the weights and the inputs. The chain rules give us the following for the weights:

\begin{equation}
\frac{\delta e}{\delta w_{ij}} = \frac{\delta e}{\delta y_j} \times \frac{\delta y_j}{\delta z_j} \times \frac{\delta z_j}{\delta w_{ij}}
\end{equation}

\noindent where $\frac{\delta e}{\delta y_j}$ is the derivative of the error function with respect to the activation of the output of the j-th neuron. The derivative of the error function with respect to the input is a bit more complex, because it required to sum over all the output neurons of the node.

\begin{equation}
\frac{\delta e}{\delta x_{i}} = \sum_{j} \frac{\delta e}{\delta y_j} \times \frac{\delta y_j}{\delta z_j} \times \frac{\delta z_j}{\delta x_i}
\end{equation}

\noindent The derivative of the error function with respect to the weights will be used to update the weights and the derivative with respect to the inputs will be used as the derivative with respect to the outputs of the previous node.

\subsection{Conv2D} \label{sec:conv2d}

This section has two objectives. First, it aims to explain the backpropagation in convolutional nodes. Second, it aims to explains how to compute the \acrshort{cpca} gradients. Lets start with the computation of the backpropagation gradients for both the weights and the inputs. The only difference for the computation of the weights gradients is the appearance of a sum over all the positions of the image where the weights have been applied:

\begin{equation} \label{eq:dwc2d}
\frac{\delta e}{\delta w_{ij}} = \sum_{k} \frac{\delta e}{\delta y_{jk}} \times \frac{\delta y_{jk}}{\delta z_{jk}} \times \frac{\delta z_{jk}}{\delta w_{ijk}}
\end{equation}

\noindent where $y_{jk}$ is the activation of the j-th neuron at the position $k$, $z_{jk}$ is the net input of the j-th neuron at the position $k$ and $w_{ijk}$ is the i-th weight of the j-th neuron at position $k$. Note that the weights are the same for all positions $k$, but $\frac{\delta z_{jk}}{\delta w_{ijk}}$ change as $k$ change. Indeed, this derivative is actually $x_{ijk}$, i.e. the input corresponding to the weight $w_{ij}$ when applied at position $k$. Figure \ref{k_weights_vs_inputs} (a) aims to clarify the meaning of the position $k$. A comparable summation appear for the derivative with respect to the inputs:

\begin{equation}
\frac{\delta e}{\delta x_{i}} = \sum_{l} \sum_{j} \frac{\delta e}{\delta y_{jl}} \times \frac{\delta y_{jl}}{\delta z_{jl}} \times \frac{\delta z_{jl}}{\delta x_{il}}
\end{equation}

\noindent where $x_{il}$ is the input whose derivative is being computed; as explained in figure \ref{k_weights_vs_inputs} (b) the $l$ index is only relevant for the weights. Indeed, $x_{il}$ is the same input for all the value of $l$, but $\frac{\delta z_{jl}}{\delta x_{il}}$ change as $l$ change. Indeed, this derivative is actually $w_{lj}$, i.e. the weight corresponding to the input $x_{i}$ at position $l$.

\begin{figure}[h]
\centering
\includegraphics[width=10cm, height=8cm]{k_weights_vs_inputs}
\caption[Backpropagation in convoluational layers: meaning of indexes]{This figure aims to clarify the meaning of the indexes $k$ and $l$. (a) The index $k$ iterate through all the positions of the input image taking steps whose size are defined by the strides of the node. (b) The index $l$ iterate through all the positions where the kernel contains the input $x_i$.}
\label{k_weights_vs_inputs}
\end{figure}

\noindent Lets now focus on the second goal of this subsection, i.e. explaining how to compute the \acrshort{cpca} gradients. The idea is to iterate through all the positions of the input image taking steps whose size are defined by the strides of the node and to compute the mean of the \acrshort{cpca} learning rule such as:

\begin{equation}
\Delta w_{ij} = \frac{\epsilon}{|K|} \sum_{k} y_{jk}(x_{ik} - w_{ij})
\end{equation}

\noindent where $w_{ij}$ is the i-th weights of the j-th neuron, $\epsilon$ is the learning rate, $|K|$ is the number of position in the image, $y_{jk}$ is the output of the neuron $j$ at position $k$ and $x_{ik}$ is the i-th input at position $k$. The constant $|K|$ can even be absorbed by the learning rate $\epsilon$ giving raise to an even simpler equation:

\begin{equation}
\Delta w_{ij} = \epsilon \sum_{k} y_{jk}(x_{ik} - w_{ij})
\end{equation}

\section{Computational Graphs}

\subsection{Graph}

This subsection aims to explains how to build a graph of computation and how the backpropagation have been implemented. The first step to create a graph is to create the nodes and set their inputs to create the graph's connectivity. It can be done as follow:

\begin{minted}[mathescape, linenos, numbersep=5pt,  frame=lines, framesep=2mm]{java}
// Create three nodes.
Node conv2d = NodesFactory.create("Conv2d");
Node flatten = NodesFactory.create("Flatten");
Node output = NodesFactory.create("Dense");

// Create the connectivity.
flatten.setInputs(conv2d);
output.setInputs(flatten);
\end{minted}

\noindent Then, the graph can be created from the graph's configuration. The only thing required for the creation of the configuration is to set the graph's outputs.

\begin{minted}[mathescape, linenos, numbersep=5pt,  frame=lines, framesep=2mm]{java}
// Create the graph's configuration.
GraphConf conf = new GraphConf().setOutputs(output);

// Create the graph according to the configuration.
Graph graph = new Graph(conf);
\end{minted}

\noindent Lets now focus on the backpropagation of the gradients through the graph, which is achieved by a recursive function presented in the pseudo code below:

\begin{minted}[mathescape, linenos, numbersep=5pt,  frame=lines, framesep=2mm]{java}
Create a map between each node and its corresponding array of gradients.
// There are as many gradients as parents of the node.
// The gradients are initially missing, i.e. null.
Compute the gradient with respect to the output of the graph.
// The gradients of the output node are now available in the map.
backpropagation(output node, map) // Start recursion

void backpropagation(node, map)
    if all the gradients of the node are available // i.e. non-null
        Compute the gradient with respect to the output of the node
        // i.e. nodeGradient = map[node].sumGradientsElementWise()
        Compute the gradient with respect to the output of the children
        // i.e. childrenGradients = node.update(nodeGradient)
        // The gradients of the node's children are now available in the map.
        for each child in node.children
            backpropagation(child, map)
\end{minted}

\begin{figure}[h]
\centering
\includegraphics[width=10cm, height=5cm]{graph_bp}
\caption[Backpropagation of the gradients in a graph of computation]{This figure shows how the gradients are backpropagated through the graph. On the left, the arrows represent the flow of data during the forward pass, i.e. graph connectivity. On the right, the arrows correspond to the computation of the gradients with respect to the nodes' inputs. The order in which the gradients are computed during the recursive process is shown by the number associate to each arrow.}
\end{figure}

\subsection{Neural Network}

The neural network class simplify the creation of sequential neural network, i.e. any stack of nodes. Also, it provides the function \mintinline{java}{addLayer} that add a node on the top of the stack. Its role is to build the underlying graph of computation.

\begin{figure}[h]
\centering
\includegraphics[width=15cm, height=5cm]{neural_network}
\caption[Interactions between neural network, nodes and dataset]{This figure shows how the neural network, the nodes and the dataset interact with each other. The first use case shows nodes being added to the stack. The second shows the training of the network where the black arrows are calls to the \mintinline{java}{activation} function and the red arrows the calls to the \mintinline{java}{update} function. Finally, the third use case shows the prediction process using the same colour code.}
\end{figure}
\newpage

\section{Zoo}

\subsection{AlexNet}

The zoo provide an implementation of many state-of-the-art models. The AlexNet is a deep ConvNet presented by Alex Krizhevsky in 2012 \cite{DBLP:journals/cacm/KrizhevskySH17}. This 
model greatly reduced the top-5 error on the ImageNet dataset, i.e. from $26\%$ to $15.3\%$. In the foundation paper the model have been trained during six days on two GTX 580. The implementation available in the zoo does not support multi-GPUs training. Table \ref{table:alexnet} and Figure \ref{fig:alexnet} show the details of the AlexNet architecture. 

\begin{table}[h!]
\centering
\begin{tabular}{  c  c  }
Layer type & Layer details\\
\hline
Conv2D & 96 filters of $11\times11$ and strides of $4\times4$ \\
MaxPooling2D & pooling kernel of $2\times2$ \\
Conv2D & 256 filters of $5\times5$ and strides of $1\times1$ \\
MaxPooling2D & pooling kernel of $2\times2$ \\
Conv2D & 384 filters of $3\times3$ and strides of $1\times1$ \\
Conv2D & 384 filters of $3\times3$ and strides of $1\times1$ \\
Conv2D & 256 filters of $3\times3$ and strides of $1\times1$ \\
MaxPooling2D & pooling kernel of $2\times2$ \\
Flatten & \\
Dense & 4096 output units \\
Dense & 4096 output units \\
Dense & 1000 output units and softmax activation
\end{tabular}
\caption[AlexNet architecture]{This table shows layers of the AlexNet architecture.}
\label{table:alexnet}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width=15cm, height=5cm]{alexnet}
\caption[AlexNet architecture]{This figure come from the AlexNet paper \cite{DBLP:journals/cacm/KrizhevskySH17}. It illustrates the AlexNet architecture and explicitly showing the delineation of responsibilities between the two GPUs. One GPU runs the layer-parts at the top of the figure while the other runs the layer-parts at the bottom. The GPUs communicate only at certain layers.}
\label{fig:alexnet}
\end{figure}

\newpage
\subsection{VGG}

Two years later Simonyan and Zisserman \cite{DBLP:journals/corr/RussakovskyDSKSMHKKBBF14} developed the VGG, which pull the top-5 error down to $8.0\%$. Figure \ref{fig:alexnet} show the details of the VGG architectures. The framework implement the configuration A, B, D and E.

\begin{figure}[h]
\centering
\includegraphics[width=11cm, height=12cm]{vgg}
\caption[VGG architectures]{This figure come from the VGG paper \cite{DBLP:journals/corr/RussakovskyDSKSMHKKBBF14}. It illustrates the VGG11, VGG13, VGG16 and VGG19 architectures. The ReLU activation function is not shown for brevity. The convolutional layer parameters are denoted as ``$conv<kernel\ size>-<number\ of\ channels>$''.}
\label{fig:vgg}
\end{figure}

\newpage
\subsection{ResNet}

In 2015, Kaiming He and al. \cite{DBLP:conf/cvpr/HeZRS16} introduced the idea of skip connections and achieve a top-5 error rate of $3.57\%$. In the foundation paper three kinds of skip connections have been proposed. The simplest one performs an identity mapping. However, some residual blocks have 64 input filters and 128 output filters, i.e. performing an identity mapping is not possible. The second kind of skip connections solve this problem by performing a zero padding. The last one perform a linear projection using a $1\times1$ convolution layer. The result of the skip connection is then added to the output of the last layer of the residual block. The final step of the residual block is to apply the ReLU activation function. The framework support the three kind of skip connections and provide an implementation of all the ResNet architectures, i.e. $ResNet[18|34|50|101|152]$.

\begin{figure}[h]
\centering
\includegraphics[width=8cm, height=4cm]{resblock}
\caption[Residual block]{This figure come from the ResNet paper \cite{DBLP:conf/cvpr/HeZRS16}. It illustrates the residual building block.}
\label{fig:resblock}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=11cm, height=5cm]{resnet}
\caption[ResNet architectures]{This figure come from the ResNet paper \cite{DBLP:conf/cvpr/HeZRS16}. It illustrates the ResNet18, ResNet34, ResNet50, ResNet101 and ResNet152 architectures.}
\label{fig:resnet}
\end{figure}

\newpage
\section{Unit testing}

The unit tests have been written using JUnit 5 and ensure that the code is working properly. The following code shows one of the 145 unit tests written:

\begin{minted}[mathescape, linenos, numbersep=5pt, frame=lines, framesep=2mm]{java}
class OperationTest {
    @Test
    void sum() {
        // Create the operation and the input.
        OperationInterface op = OpsFactory.create("Operation", "cpu");
        ArrayPtr x = create(new float[][]{{1, 2}, {2, 1}});
        // Check the validity of the sum, i.e. 1 + 2 + 2 + 1 = 6.
        assertEquals(6, op.sum(x));
    }
}
\end{minted}

\noindent Since, this framework is about backpropagation it is important to ensure that the gradients are correctly computed. The unit tests are implementing a technique named numerical gradient checking to check the correctness of the gradients. The key is to compute an approximation of the derivative with respect to a variable such as:
\begin{equation}
g(x) = \frac{df(x)}{dx} = \lim_{k\to0} \frac{f(x + k) - f(x - k)}{2k}
\end{equation}
where $g$ is the derivative, $f$ is the function and $x$ is variable.
Figure \ref{fig:gradient_checking} illustrates the idea behind this method for the function: $f(x) = x^{2}$. A neural network can be see as a parameterized function $f$ that transform some imput $x$ into an output $f(x, \theta)$ based on the weights $\theta$. In this context the variable can either be $x$ or $\theta$. The approximation of the derivative is then compared with the actual value computed by the neural network.

\begin{figure}[h]
\centering
\includegraphics[width=8cm, height=5cm]{gradient_checking}
\caption[Illustration of the numerical gradient checking method]{Illustration of the numerical gradient checking method.}
\label{fig:gradient_checking}
\end{figure}

\section{\acrlong{gpu}}

\subsection{Core concepts} \label{sec:core_concepts}

The \acrshort{gpu} was initially designed for graphics application where the need for parallelization is huge due to the large number of pixels to process. In order to provide such parallelization \acrshort{gpu} are generally composed of hundreds if not thousands of cores. Each core can then processes data in parallel. Originally, the API available to run code on GPU was OpenGL and DirectX, which was specialize in graphics computation. It only in November 2006 that NVIDIA unveiled the GeForce 8800 GTX built with the CUDA architecture enabling general-purpose computation.
\\

\begin{figure}[h]
\centering
\includegraphics[width=10cm, height=6cm]{gpu_architecture}
\caption[Illustration of the GPU architecture]{Illustration of the GPU architecture. The red arrows link the threads to the memory they can access. The global memory is the basic memory accessible from every threads. The constant memory is similar to the global memory, but is accessible in read-only. Finally, the shared memory is shared amoung threads of the same block. Also, the need to syncronize the threads appears for any application that needs to reduce the shared memory, e.g. computing the sum of all the elements. This can be achieve by calling the function \mintinline{cuda}{__syncthreads()} to ensure that all threads finished their job.}
\label{fig:gpu_architecture}
\end{figure}

\noindent Figure \ref{fig:gpu_architecture} shows the GPU architecture. The following aims to explain how to use the CUDA API. The first thing to understand is the difference between the host and the device. The host refer to the CPU and the device refer to the GPU. A kernel is a function which run on the device and can be called from the host. A kernel does not return any value and must be annotate with the qualifier \mintinline{cuda}{__global__}, which tell the NVIDIA compiler that the function is actually a kernel.

\begin{minted}[mathescape, linenos, numbersep=5pt,  frame=lines, framesep=2mm]{cuda}
__global__ void kernel(int parameter, float *result) { ... }
\end{minted}

\noindent Another important qualifier is \mintinline{cuda}{__device__}, which tell the NVIDIA compiler that the function will be executed on the device. A device function can be call from any kernel or device function.

\begin{minted}[mathescape, linenos, numbersep=5pt,  frame=lines, framesep=2mm]{cuda}
__device__ float device_function(int parameter) { ... }
\end{minted}

\noindent The last thing to understand is how the threads are organized. Indeed, threads are arranged into blocks and blocks are packed to form the grid. Blocks and threads can have indexes in up to three dimensions. The figure \ref{fig:grid} shows a visual representation of the grid.
\\

\begin{figure}[h]
\centering
\includegraphics[width=6cm, height=5.5cm]{grid}
\caption[Illustration of the GPU grid]{An example grid where both blocks and threads have 2D indexes. The purple arrows show the blocks dimensions and the orange arrows show the threads dimensions.}
\label{fig:grid}
\end{figure}

\noindent Finally, the indexes of the thread and block being processed are available in the kernel through the variables: \mintinline{cuda}{threadIdx.x}, \mintinline{cuda}{threadIdx.y}, \mintinline{cuda}{threadIdx.z}, \mintinline{cuda}{blockIdx.x}, \mintinline{cuda}{blockIdx.y} and \mintinline{cuda}{blockIdx.z}. Also, the number of threads and blocks are available in the kernel through the variables: \mintinline{cuda}{blockDim.x}, \mintinline{cuda}{blockDim.y}, \mintinline{cuda}{blockDim.z}, \mintinline{cuda}{gridDim.x}, \mintinline{cuda}{gridDim.y} and \mintinline{cuda}{gridDim.z}.

\subsection{Application to neural networks}

Section \ref{sec:core_concepts} has presented all the main concepts of CUDA C, the following describes how it have been used to speed up the framework. First, it is worth noting that some computer does not have \acrshort{gpu}. Also, it is useful to provide a factory that instantiate either the \acrshort{cpu} or the \acrshort{gpu} implementation depending on the hardware available in the computer. The following line load the correct implementation for the $Conv2D$ node:

\begin{minted}[mathescape, linenos, numbersep=5pt,  frame=lines, framesep=2mm]{java}
Node conv_2d = NodesFactory.create("Conv2d");
\end{minted}

\noindent \textcite{DBLP:journals/tjs/BritoFCSWMF16} show that \acrshort{gpu}s can be successfully used to speed up artificial neural networks. Even if the framemork has a \acrshort{gpu} implementation for all nodes presented in section \ref{sec:nodes}. The following paragraphs focus on the backward pass in 2D convolutional node. Indeed, it is one of the most challenging to understand and a clear explanation will provide great insight for using \acrshort{gpu} to speed up neural networks. This kernel is using all the blocks and threads dimensions, i.e. all the following indexes are used: \mintinline{cuda}{threadIdx.x}, \mintinline{cuda}{threadIdx.y}, \mintinline{cuda}{threadIdx.z}, \mintinline{cuda}{blockIdx.x}, \mintinline{cuda}{blockIdx.y} and \mintinline{cuda}{blockIdx.z}. The blocks' indexes are used to go through all the weights that are stored in a 4D array, i.e. the first dimension corresponds to the number of neurons in the layer, the second to the number of input channels, and finally, the third and fourth to the kernel's width and height respectively. Because there is only three blocks indexes (i.e. $x$, $y$ and $z$), the first one is used to go through the two first dimensions. As explained in section \ref{sec:conv2d} the computation of the derivatative with repect to the weights required to sum the gradients over all positions $k$, cf. equation \ref{eq:dwc2d}. As previously explained, each block is responsible to compute the derivative with respect to one weight, i.e. all weights are computed in parallel. Moreover, the threads of each block proccess several part of the images at the same time. The threads' $x$ index iterate through all the images of the batch being processed, the $y$ index through all the vertical positions and $z$ index through all the horizontal positions. The shared memory is used by each thread to store the sum of all the positions it is responsible for. Then, the \mintinline{cuda}{__syncthreads()} function is called to synchronize all the threads. Finally, the first thread of each block sum the elements of the shared memory and place the result in the output buffer. The figure \ref{fig:conv2d_backpropagation} illustrate this process for a batch of two images.

\begin{figure}[h]
\centering
\includegraphics[width=9cm, height=7cm]{conv2d_backpropagation}
\caption[Illustration of the GPU-enabled backpropagation in convolutional layer]{This figure illustrate the GPU-enabled backpropagation in the convolutional layer. For simplicity this figure assumes two images with only one channel and two kernels of size $2\times2$. a) shows the meaning of the blocks and threads indexes. b) shows how the summation over all the positions is performed in parallel. This assume a $2\times2$ stride and a $1\times2\times2$ grid of blocks. The first thread, i.e. purple, is responsible for the top right positions within each images. The last thread, i.e. orange, is responsible for the bottom left positions within each images. c) shows the sum of the elements in the shared memory performed by the first thread of each block after the call to the \mintinline{cuda}{__syncthreads()} function.}
\label{fig:conv2d_backpropagation}
\end{figure}

\subsection{Optimisation of data movement} \label{opti_data_move}

This subsection explains an optimisation which enabled the framework to run three time faster. It aims to highlight the amount of time lost when the data is moved back and forth between the CPU and the GPU. Indeed, the first implementation of the framework was copying the data from the CPU to the GPU before each kernel call. After the call, the result was copied backward, i.e. from the GPU to the CPU.
\\

\noindent The new implementation has introduced the $ArrayPtr$ class. This class aims to abstract the location of the data. Because the $ArrayPtr$ can be passed directly from one node to another, the data can stay on the device. The time that was needed to move the data back and forth between the host and the device is saved.

\chapter{Results and Analysis} \label{result_and_analysis}

\section{\acrshort{gpu} vs \acrshort{cpu}}

\subsection{Hardware specifications}

This subsection aims to give the hardware specifications of the CPU and GPU used for the benchmarks. The GPU implementations have been tested on a Pascal Tesla P100, which belongs to the Pascal generation and deliver up to 10.6 TFLOPS of single precision (FP32) performance. The CPU implementation have been tested on a Intel Xeon E5520 CPUs running at 2.27GHz, which has 4 cores, i.e. 8 threads.

\subsection{Layers}

In this section the CPU and GPU implementation of six layers are presented. The speed of each layer have been evaluated for 100, 1000 and 10000 iterations. One iteration corresponds to both the forward and the backward pass. The iteration is performed on ten MNIST images, which correspond to an array of $10\times1\times28\times28$ for the $MaxPooling2D$ and $Conv2D$ layers as well as an array of $10\times784$ for the $Dense$ layer. Each experiment have been run ten times and the tables below show the mean of execution time and the standard deviation (STD).
\newline
\newline
\noindent The first interresting thing to look at is that some layers are more suited to the GPU implementation. Indeed, tables \ref{tab:conv2d_res} and \ref{tab:max_pooling_2d_res} show that the GPU-enabled Conv2D and MaxPooling layers are up to 2874 and 5990 times faster than their CPU counterparts. On the other hand, table \ref{tab:dense_res} shows that the GPU implementation of the Dense layer is only up to 12 times faster. This is due to the the properties of the problems being solved efficently by the GPU. The first kind of layers require a huge amount of  computation that can be computed in parallel, i.e. at different position of the image. The Dense layer is more difficult to parallelize, because each neuron needs to compute a weighted sum of its inputs. This weighted sum can be seen as a global statistic difficult to parallelize. It also require threads synchronization, which leading to a lost of performance.
\newline
\newline
\noindent Second, the table \ref{table:CNN_vs_HCNN} shows the additional time required to compute the CPCA gradients and kWTA activation. This extra-time seems to grow with the number of iterations. The convolutional layer is up to 1.5 times faster than its conterpart with CPCA and kWTA.

\begin{table}[h!]
\centering
\begin{tabular}{ c | c c | c c | c c }
Iterations & Device time (ms) & STD & Host time (ms) & STD & Speed up & p-value\\
\hline
100   & 295  & 46.58 & 171996   & 4380   & 583 & $<$ 0.0001 \\
1000  & 931  & 61.17 & 1651739  & 49192  & 1774 & $<$ 0.0001 \\
10000 & 5967 & 96.09 & 16600000 & 622521 & 2782 & $<$ 0.0001
\end{tabular}
\caption[Speed benchmark: Conv2D layer]{Speed comparison between CPU and GPU implementation of the Conv2D layer.}
\label{tab:conv2d_res}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{ c | c c | c c | c c }
Iterations & Device time (ms) & STD & Host time (ms) & STD & Speed up & p-value\\
\hline
100   & 289  & 53    & 191000   & 5074   & 661 & $<$ 0.0001 \\
1000  & 954  & 40.05 & 1830000  & 19134  & 1918 & $<$ 0.0001 \\
10000 & 6298 & 94.26 & 18100000 & 435109 & 2874 & $<$ 0.0001
\end{tabular}
\caption[Speed benchmark: Conv2D layer with kWTA]{This table shows the speed benchmark between the CPU and the GPU implementation of the Conv2D layer with kWTA.}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{ c | c c | c c | c c }
Iterations & Device time (ms) & STD & Host time (ms) & STD & Speed up & p-value\\
\hline
100   & 359  & 42.37  & 183000   & 4587   & 510 & $<$ 0.0001 \\
1000  & 1270 & 60.58  & 1730000  & 38130  & 1362 & $<$ 0.0001 \\
10000 & 8562 & 108.50 & 17100000 & 216927 & 1997 & $<$ 0.0001
\end{tabular}
\caption[Speed benchmark: Conv2D layer with CPCA]{This table shows the speed benchmark between the CPU and the GPU implementation of the Conv2D layer with CPCA.}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{ c | c c | c c | c c }
Iterations & Device time (ms) & STD & Host time (ms) & STD & Speed up & p-value\\
\hline
100   & 339  & 41.05  & 198000   & 3324   & 584 & $<$ 0.0001 \\
1000  & 1241 & 64.08  & 1880000  & 32474  & 1515 & $<$ 0.0001 \\
10000 & 8954 & 119.40 & 19000000 & 319616 & 2122 & $<$ 0.0001
\end{tabular}
\caption[Speed benchmark: Conv2D layer with CPCA and kWTA]{This table shows the speed benchmark between the CPU and the GPU implementation of the Conv2D layer with CPCA and kWTA.}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{ c | c c | c c | c c }
Iterations & Device time (ms) & STD & Host time (ms) & STD & Speed up & p-value\\
\hline
100   & 115  & 11.80 & 1360  & 294.70 & 12 & $<$ 0.0001 \\
1000  & 302  & 15.72 & 3090  & 409.92 & 10 & $<$ 0.0001 \\
10000 & 1969 & 24.89 & 13100 & 612.96 & 7 & $<$ 0.0001
\end{tabular}
\caption[Speed benchmark: Dense layer]{Speed comparison between CPU and GPU implementation of the Dense layer.}
\label{tab:dense_res}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{ c | c c | c c | c c }
Iterations & Device time (ms) & STD & Host time (ms) & STD & Speed up & p-value\\
\hline
100   & 12  & 2.72  & 26600   & 585   & 2217 & $<$ 0.0001 \\
1000  & 58  & 10.58  & 238000  & 3296  & 4103 & $<$ 0.0001 \\
10000 & 394 & 20.07 & 2360000 & 37037 & 5990 & $<$ 0.0001
\end{tabular}
\caption[Speed benchmark: MaxPooling2D layer]{Speed comparison between CPU and GPU implementation of the MaxPooling2D layer.}
\label{tab:max_pooling_2d_res}
\end{table}

\begin{table}[!htb]
\centering
\begin{tabular}{ c | c c | c c | c c }
Iterations & Conv2D (ms) & STD & Hebbian Conv2D (ms) & STD & Speed up & p-value\\
\hline
100   & 295  & 46.58 & 339  & 41.05  & 1.1492 & 0.0379 \\
1000  & 931  & 61.17 & 1241 & 64.08  & 1.3330 & $<$ 0.0001 \\
10000 & 5967 & 96.09 & 8954 & 119.40 & 1.5006 & $<$ 0.0001
\end{tabular}
\caption[Speed benchmark: Computational cost of CPCA and kWTA]{This table shows the computational cost of CPCA and kWTA in the GPU implementation of the Conv2D layer. The third line can be read as follows: for 10000 iterations the convolutional layer is 1.5 times faster than its conterpart with CPCA and kWTA.}
\label{table:CNN_vs_HCNN}
\end{table}
\FloatBarrier

\subsection{Networks}

In this section the CPU and GPU implementation of four networks are presented. All networks are composed of four layers:

\begin{itemize}
	\item \textbf{Conv2D} with 8 filters of $2\times2$, strides of $2\times2$ and a ReLU activation function;
	\item \textbf{MaxPooling2D} with $2\times2$ pooling kernel;
	\item \textbf{Flatten};
	\item \textbf{Dense} with 10 output units.
\end{itemize}

\noindent The difference between the four networks is that the first does not use neither CPCA nor kWTA, the second use kWTA, the third use CPCA and the fourth use both CPCA and kWTA. Each network have been tested on 1 and 3 epochs. One epoch is defined by the training of the network over the entire MNIST dataset loaded by batch of twenty examples. Each experiment have been run ten times and the tables below show the mean of execution time and the standard deviation (STD).

\begin{table}[h!]
\centering
\begin{tabular}{ c | c c | c c | c c }
Epochs & Device time (ms) & STD & Host time (ms) & STD & Speed up & p-value\\
\hline
1 & 5908  & 91.59  & 13600000 & 245293 & 2302 & $<$ 0.0001 \\
3 & 13777 & 152.16 & 42100000 & 357999 & 3056 & $<$ 0.0001
\end{tabular}
\caption[Speed benchmark: Conv2D network]{This table shows the benchmark between the CPU and the GPU implementation of the Conv2D network.}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{ c | c c | c c | c c }
Epochs & Device time (ms) & STD & Host time (ms) & STD & Speed up & p-value\\
\hline
1 & 5933  & 62.66  & 14200000 & 281201 & 2393 & $<$ 0.0001 \\
3 & 14126 & 134.05 & 44100000 & 703370 & 3122 & $<$ 0.0001
\end{tabular}
\caption[Speed benchmark: Conv2D network with kWTA]{This table shows the benchmark between the CPU and the GPU implementation of the Conv2D with kWTA network.}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{ c | c c | c c | c c }
Epochs & Device time (ms) & STD & Host time (ms) & STD & Speed up & p-value\\
\hline
1 & 7150  & 246.93 & 15900000 & 662694  & 2224 & $<$ 0.0001 \\
3 & 17522 & 438.07 & 49100000 & 2540731 & 2802 & $<$ 0.0001
\end{tabular}
\caption[Speed benchmark: Conv2D network with CPCA]{This table shows the benchmark between the CPU and the GPU implementation of the Conv2D with CPCA network.}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{ c | c c | c c | c c }
Epochs & Device time (ms) & STD & Host time (ms) & STD & Speed up & p-value\\
\hline
1 & 7364  & 210.29 & 14900000 & 400573  & 2023 & $<$ 0.0001 \\
3 & 17924 & 318.31 & 44700000 & 1301869 & 2494 & $<$ 0.0001
\end{tabular}
\caption[Speed benchmark: Conv2D network with CPCA and kWTA]{This table shows the benchmark between the CPU and the GPU implementation of the Conv2D with kWTA and CPCA network.}
\end{table}
\newpage

\section{\acrshort{hcnn} vs \acrshort{cnn}}

\subsection{Accuracy and Learning speed} \label{sec:accuracy_and_learning_speed}

\subsubsection{Model intuition}

In this section the HCNN and CNN are compared in terms of accuracy and learning speed. In order to achieve a fair comparison, both network architectures are the same and only the number of winners and the ratio of CPCA gradients is modified. Additionaly, the size of the training set as well as the number and size of filters can be changed between the experiments. The default filters size is $2\times2$; if the filters size is modified, then it will be explicitly specified. All the networks tested have the following four layers:

\begin{itemize}
	\item \textbf{Conv2D} with strides of $2\times2$ and a ReLU activation function;
	\item \textbf{MaxPooling2D} with $2\times2$ pooling kernel;
	\item \textbf{Flatten};
	\item \textbf{Dense} with 10 output units.
\end{itemize}

\noindent Each of the following experiment have been run 10 times and at each time step the mean and standard deviation have been computed. Note that the STD is generally quite large, this is explained by the fact that all the weights of the networks are randomlly initialised before each experiment, i.e. the initial networks parameters can be very different from one experiment to another.
\newline

\noindent There are three possible outcomes to each experiment, i.e. the HCNN can be worse, equivalent or better than the CNN. The outcome is consider equivalent if the test is not statisticaly significiant at the level of $0.05$. The outcome is consider as worse or better if the test is statisticaly significiant at the level of $0.05$. Figure \ref{fig:hcnn_worse} shows an example of the worse cases and table \ref{table:hcnn_worse} shows all the experiments' parameters for which this behaviour have been observed. Note that the number of iterations from which the accuracy starts to decrease may vary from one experiment to another, i.e. around 500 iterations in figure \ref{fig:hcnn_worse} (a) and from the beginning in figure \ref{fig:hcnn_worse} (b).

\begin{figure}[h]
\centering
\includegraphics[width=13cm, height=4cm]{hcnn_worse}
\caption[Illustration of two experiments where the CNN is better than the HCNN.]{(a) comparison between the CNN and the HCNN with 8 filters, 1 winner and a CPCA ratio of 0.05. (b) comparison between the CNN and the HCNN with 32 filters, 32 winners and a CPCA ratio of 0.001. Those experiments have been run on 240 and 60,000 examples respectively. In those cases the results of the HCNN are lower than the CNN results.}
\label{fig:hcnn_worse}
\end{figure}

\begin{table}[h!]
\centering
\begin{tabular}{ c c c c c c }
Dataset size & Number of filters & Number of winners & CPCA ratio & Breaking  point & p-value\\
\hline
240 & 8 & 1 & 0.05 & 500 & 0.0015\\
60,000 & 32 & 8 & 0.001 & 0 & 0.0242\\
60,000 & 32 & 32 & 0.001 & 0 & 0.0018
\end{tabular}
\caption[Benchmark where the \acrshort{cnn} is better than the \acrshort{hcnn}.]{This table shows the list of experients where the results of the \acrshort{cnn} are better than the results of the \acrshort{hcnn}. The ``breaking point'' is the number of iterations from which the accuracy starts to decrease.}
\label{table:hcnn_worse}
\end{table}
\newpage

\noindent Figure \ref{fig:hcnn_equi} shows examples where the results of the \acrshort{cnn} and the \acrshort{hcnn} are equivalent and table \ref{table:hcnn_equi} shows all the experiments' parameters for which this behaviour have been observed.

\begin{figure}[h]
\centering
\includegraphics[width=12cm, height=4.5cm]{hcnn_equi}
\caption[Illustration of two experiments where the HCNN and CNN results are equivalent.]{(a) comparison between the CNN and the HCNN with 8 filters, 2 winners and a CPCA ratio of 0.001. (b) comparison between the CNN and the HCNN with 8 filters, 1 winner and a CPCA ratio of 0.001. Those experiments have been run on only 500 and 240 examples respectively.}
\label{fig:hcnn_equi}
\end{figure}

\begin{table}[h!]
\centering
\begin{tabular}{ c c c c c }
Dataset size & Number of filters & Number of winners & CPCA ratio & p-value\\
\hline
240 & 8 & 1 & 0.001 & 0.1873\\
240 & 8 & 1 & 0.005 & 0.3883\\
240 & 16 & 2 & 0.001 & 0.5144\\
500 & 8 & 2 & 0.001 & 0.8463\\
500 & 16 & 2 & 0.001 & 0.0539\\
60,000 & 8 & 1 & 0 & 0.5204\\
60,000 & 8 & 1 & 0.005 & 0.0877\\
60,000 & 8 & 1 & 0.01 & 0.7154\\
60,000 & 8 & 2 & 0.01 & 0.1266\\
60,000 & 8 & 2 & 0.005 & 0.6469\\
60,000 & 8 & 8 & 0.001 & 0.8558\\
60,000 & 32 & 8 & 0 & 0.7912
\end{tabular}
\caption[Benchmark where the \acrshort{hcnn} and the \acrshort{cnn} are equivalents.]{This table lists the experients where the \acrshort{hcnn} and \acrshort{cnn} give equivalents results.}
\label{table:hcnn_equi}
\end{table}

\noindent Figure \ref{fig:hcnn_better} shows examples where the results of the \acrshort{hcnn} are better than the \acrshort{cnn} and table \ref{table:hcnn_better} shows all the experiments' parameters for which this behaviour have been observed.

\begin{figure}[h]
\centering
\includegraphics[width=12cm, height=4.5cm]{hcnn_better}
\caption[Illustration of two experiments where the HCNN is better than the CNN.]{(a) comparison between the CNN and the HCNN with 8 filters, 1 winner and a CPCA ratio of 0.001. (b) comparison between the CNN and the HCNN with 16 filters, 1 winner and a CPCA ratio of 0.001. Those experiments have been run on 60,000 and 240 examples respectively.}
\label{fig:hcnn_better}
\end{figure}

\begin{table}[h!]
\centering
\begin{tabular}{ c c c c c }
Dataset size & Number of filters & Number of winners & CPCA ratio & p-value\\
\hline
240 & 16 & 1 & 0.001 & 0.0004\\
500 & 8 & 1 & 0.001 & 0.0219\\
500 & 16 & 1 & 0.001 & 0.0005\\
60,000 & 8 & 1 & 0.001 & 0.0255
\end{tabular}
\caption[Benchmark where the \acrshort{hcnn} is better than the \acrshort{cnn}.]{This table shows the experients where the \acrshort{hcnn} give better results than the \acrshort{cnn}.}
\label{table:hcnn_better}
\end{table}
\newpage

\noindent This sequence of tests ask the following research questions:

\begin{itemize}
	\item What are the causes of the breaking point?
	\item Is the HCNN able to achieve high accuracy on small datasets?
	\item Is the conditioning function essential to the learning?
\end{itemize}
\newpage

\paragraph{Question 1:} What are the causes of the breaking point?
\newline
\newline
\texttt{Hypothesis 1:} A reasonable CPCA ratio is around 0.001 and high ratio leads to breaking point.
\newline

\noindent In order to test this hypothesis, two HCNN have been tested with various value of the CPCA ratio. The results are presented in figure \ref{fig:hcnn_breaking_point}.

\begin{figure}[h]
\centering
\includegraphics[width=15cm, height=15.5cm]{longRun-f_16-ds_250}
\caption[Experiment 1: What are the causes of the breaking point?]{The first column compares the CNN and the HCNN with 16x2x2 filters and 1 winner. The second column compares the CNN and the HCNN with 16x3x3 filters and 1 winner. Each row correspond to one value of the CPCA ratio, i.e. starting from the top to the bottom: 0.001, 0.005 and 0.01. Those experiments have been run on 240 examples.}
\label{fig:hcnn_breaking_point}
\end{figure}

\noindent \textit{\underline{Answer 1.1:} Considering the results there is no reason to believe that hypothesis 1 is not correct. Indeed, both HCNN are stable with a ratio of 0.001 and exhibit breaking point behaviour as the CPCA ratio increase.}
\newpage

\noindent \texttt{Hypothesis 2:} A reasonable number of winners is around 1.
\newline
\noindent \texttt{Hypothesis 3:} A high number of winners leads to breaking point.
\newline
\newline
\noindent In order to test this hypothesis, one HCNN have been tested with various value of the $k$. The results are presented in figure \ref{fig:hcnn_breaking_point_kwta}.

\begin{figure}[h]
\centering
\includegraphics[width=14.5cm, height=16cm]{longRun-f_16-ds_250-kwta}
\caption[Experiment 2: What are the causes of the breaking point?]{This figure compares the CNN and the HCNN with 16x2x2 filters and a CPCA ratio of 0.001. Each image correspond to one value of the $k$, i.e. starting from 1 to 15 using a step size of 2. Those experiments have been run on 240 examples.}
\label{fig:hcnn_breaking_point_kwta}
\end{figure}

\noindent \textit{\underline{Answer 1.2:} Considering the results there is no reason to believe that hypothesis 2 is not correct. Indeed, when the number of winners is equal to 1 the HCNN's accuracy is superior to the CNN.}
\newline
\noindent \textit{\underline{Answer 1.3:} Considering the results the hypothesis 3 seems to incorrect. Indeed, as the number of winners increases the accuracy of the network remains stable and no breaking point is observed.}
\newpage

\paragraph{Question 2:} Is the HCNN able to achieve high accuracy on small datasets?
\newline
\newline
\noindent In order to answer this question, one HCNN have been tested with various dataset size. The results are presented in figure \ref{fig:hcnn_small_data_set}. Note that the number of iterations may change from one experiment to another. Surprisingly, this set of tests also support the hypothesis that the dataset size can lead to breaking point. However, this is not the subject of this section and will not be developed.

\begin{figure}[h]
\centering
\includegraphics[width=15cm, height=15cm]{longRun-f_16}
\caption[Experiment 3: Is the HCNN able to achieve high accuracy on small datasets?]{This figure compares the CNN and the HCNN with 16x2x2 filters, 1 winner and a CPCA ratio of 0.001. Each image correspond to one dataset size.}
\label{fig:hcnn_small_data_set}
\end{figure}

\noindent \textit{\underline{Answer 2:} In this answer the accuracy is considered as high when the HCNN outperform the CNN. In order to avoid the drop of accuracy, it is possible to use the early-stopping method during the HCNN learning. Under this setting , the results show that there is no reason to reject the hypothesis that the HCNN is able to achieve high accuracy on small dataset.}
\newline
\newline
\paragraph{Question 3:} Is the conditioning function essential to the learning?
\newline
\newline
\noindent In order to answer this question, all the successful experiments presented in table \ref{table:hcnn_better} have been re-run without kWTA. The results are presented in figure \ref{fig:hcnn_conditioning_func} and table \ref{table:conditioning_func}.

\begin{figure}[h]
\centering
\includegraphics[width=12cm, height=4.5cm]{hcnn_conditioning_func}
\caption[Experiment 4: Is the conditioning function essential to the learning?]{(a) and (b) comparison between the CNN and the HCNN with 16 filters, 16 winner and a CPCA ratio of 0.001. Those experiments have been run on 500 and 240 examples respectively.}
\label{fig:hcnn_conditioning_func}
\end{figure}

\begin{table}[h!]
\centering
\begin{tabular}{ c c c c c }
Dataset size & Number of filters & Number of winners & CPCA ratio & p-value\\
\hline
240 & 16 & 16 & 0.001 & 0.9496\\
500 & 8 & 8 & 0.001 & 0.7340\\
500 & 16 & 16 & 0.001 & 0.9246\\
60,000 & 8 & 8 & 0.001 & 0.9172
\end{tabular}
\caption[Experiment: Is the conditioning function essential to the learning?]{This table shows the result of the experiments that previously lead to better outcomes. Those experiments have been re-run without the conditioning function, i.e. without kWTA.}
\label{table:conditioning_func}
\end{table}

\noindent \textit{\underline{Answer 3:} The lack of conditioning function does not make the HCNN unable to learn, but the HCNN lost its superiority. Under those settings, the HCNN seems to learn using only backpropagation, i.e. like a CNN.}

\newpage
\subsection{Weights analysis}

This section aims to compare the weights learned by the CNN and the HCNN. The hope was that the HCNN will have learn features based on the data, i.e. with clear patterns visible in the filters. Unfortunately, figure \ref{fig:trained_weights} tells a different story. The weights of both models seem to be similar and no patterns emerges. Likewise, the figure \ref{fig:trained_weights_20} shows that increasing the size of the receptive field, i.e. the kernels' size, does not lead to better weights patterns. Those results might be due to the small CPCA ratio used during the training, i.e. $0.001$. Unfortunately, as highlighted in section \ref{sec:accuracy_and_learning_speed} bigger ratio lead to poor results. Additionaly, the second result might also be caused by the low accuracy achieve by the network, i.e. $0.58$.

\begin{figure}[h]
\centering
\includegraphics[width=16cm, height=3.5cm]{trained_weights}
\caption[Illustration of the CNN's and HCNN's weights obtained after training (5x5 kernels).]{This figure shows the weights obtained after training both the CNN and the HCNN. The first row present the 8 filters of size 5x5 of the CNN after the training procedure. The second row present the 8 filters of the HCNN with 1 winner and a CPCA ratio of 0.001. Black pixels correspond to the smallest weights and white pixels correspond to the biggest ones.}
\label{fig:trained_weights}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=16cm, height=7cm]{trained_weights_20}
\caption[Illustration of the HCNN's weights after training (20x20 kernels).]{This figure shows the weights obtained by training the HCNN with kernels of size 20x20. The large kernels' size makes the tasks difficult to solve and the accuracy of the network never exceeded $0.58$. The result of the CNN with 20x20 kernels are not presented, because the network was unable to reach $0.4$ of accuracy.}
\label{fig:trained_weights_20}
\end{figure}

\chapter{Framework limitations} \label{framework_limitation}

This chapter aims to explains the current limitations of the framework and their implications. The first limitation is that the graph does not support multiple inputs and outputs. This become a problem to implement models such as the GoogleNet, which require three time the same output as shown in figure \ref{fig:googlenet}.

\begin{figure}[h]
\centering
\includegraphics[width=16cm, height=5cm]{googlenet}
\caption[GoogleNet architecture]{This figure come from the GoogleNet paper \cite{DBLP:conf/cvpr/SzegedyLJSRAEVR15} and illustrate the GoogleNet architecture. Each softmax function is linked to the label to predict, i.e. multi-output graph of computation.}
\label{fig:googlenet}
\end{figure}
\noindent The max and mean pooling layers does not support overlapping. Indeed, the framework only support pooling where the strides have the same size as the pooling kernel. This introduce slight differences between the original implementation of the AlexNet and the zoo implementation.
\newline
\newline
\noindent Local response normalisation, weights decay and dropout are also missing from the VGG implmentation available in the zoo. The UNet and LinkNet used in image segmentation cannot be implemented, because the deconvolutional layer is still unavailable.
\newline
\newline
\noindent Finally, additional work is needed in order to allow very big models such as the ResNet152 to run properly. Indeed, this kind of model reach the limit of memory available on the device and additional transfers of data must be implemented to avoid that the program crash.

\chapter{Conclusion and Future Research} \label{conclusion}

\section{Objectives}

This dissertation has three objectives. The first was to propose a variation of the convolutional layer using a mixture of supervised and unsupervised learning. This variation have been describe in chapter \ref{model_and_task_learning}, which explains how to combine CPCA, kWTA and backpropagation. The two last goals were to implement the backpropagation in a graph of computation from scratch and to harness the power of GPU programming. Chapter \ref{model_implementation} describes the implementation of the backpropagation as well as the graph logic and quickly present the state-of-the-art models implemented in the framework's zoo. Addtionally, this chapter explained how to use JUnit to properly test the framework's logic and the backpropagation of the gradients. This chapter ended with the presentation of the core concepts of GPU programming and explained how to use C CUDA to achieve hardware acceleration for neural networks. 

\section{Discussion and future work}

Chapters 5 and 6 presented the benchmarks results and the current framemork limitation. In this final discussion those results and limitations are turned into the future directions of research and ideas of improvement.
\newline
\newline
Lets us begin with the ideas of improvement for the framework. The first step that should be undertaken is to solidify the current base of code and implement the missing features. The missing features include max and mean pooling with overlapping, local response normalisation, weights decay and dropout. In addition to those small differences related to the zoo models, the memory management should be improved. Indeed, when the device run out of memory the unused data should be tranferred back to the host. This will enable very big model such as the ResNet152 to run successfully. As explained in section \ref{opti_data_move}, data movement should be minimized to avoid additional computational cost. Another important mutation is to re-implement the framework as a C++ API. This will lead to a considerable speed up and allow any other languages such as Python, Go and Ruby to use the framework by calling the underlying API methods. Afterward, the next steps will be to add support for graph with multiple inputs and outputs, an interface for optimizers and new layers, e.g. deconvolutional and reccurent layers. 
\newline
\newline
Lets us now focus on the discussion of the HCNN and the future direction of research. The first point that will be discuss is research towards the elimination of the breaking points. As highlighted in section \ref{sec:accuracy_and_learning_speed} high CPCA ratio might lead to a drop of the network's accuracy. In order to avoid such drop, it might be interesting to implement early stopping or time-based decay of the CPCA ratio, i.e. $r = \frac{r_0}{1 + k \times t}$ where $r_0$ is the initial CPCA ratio, $r$ is the new CPCA ratio, $k$ is an hyperparameter and $t$ is the iteration number. This should remove the drop of accuracy, but also introduce a new hyperpameter $k$.
\newline
\newline
This lead us to the second direction of research that aims to remove the hyperparameters $r_0$ and $k$ introduced by the HCNN. Let'us remember than the CPCA gradients are supposed to guide the learning of the network by adding bias towards the data. This implicitly means that once the tasks have been learn, i.e. when the backpropagation gradients are small, then the CPCA gradients are no longer needed. Instead of using the hyperparameters $r_0$ and $k$ to compute $r$, it might be interesting to express $r$ as a function of the backpropagation gradients. If this is successful then it will makes the algorithm self adaptative and the hyperparameters will no longer be necessary.
\newline
\newline
As developped in the question 2 of section \ref{sec:accuracy_and_learning_speed}, the HCNN performs better than the CNN on small datasets. However, the Dense layer of the network tested was only learning using backpropagation. There is no reason to believe that the Hebbian Dense layer, i.e. a Dense layer using CPCA and kWTA, will lead to poor accuracy on small dataset. Thus, the third direction of research is to implementation and study Hebbian learning in other types of layers, e.g. Dense or reccurent layers. Also, in section \ref{sec:accuracy_and_learning_speed}, the HCNN was compared with a CNN without regularisation or dropout. On a small dataset it seems reasonable to believe that regularisation and dropout can help the learning by keeping the weights small and reducing the interdependency between the neurons. Indeed, one of the strength of the CPCA with kWTA is that only a subset of the neurons remain active and learn. Since, dropout works similarly it might be interesting to compare the HCNN and a CNN with dropout or/and regularisation.
\newline
\newline
Finally, the fourth and last direction of research of this paper is to study the impact of the Hebbian convolutional layer in more complex networks and on other datasets. This aims to study the generalisation of the method to bigger architectures, e.g. ResNet or AlexNet, and bigger datasets such as ImageNet. If the Hebbian convolutional layer demonstrate the ability to generalize to the ResNet architecture on the ImageNet dataset, then it might become the new state-of-the-art for image classification.

\printbibliography

\end{document}
